{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2e9b3e6d-d507-46e3-806a-1e5a9f7f3eaf",
   "metadata": {},
   "source": [
    "# 1.4. AirBnB Project Template\n",
    "\n",
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28ed7c80-1bbd-4db5-8113-67f5ba78551b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "# MULTI LAYER PERCEPTRON - ARTIFICIAL NEURAL NETWORK - OPTIMIZED NETWORK\n",
    "#  \n",
    "\n",
    "import numpy as np # conda install numpy\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt # conda install matplotlib\n",
    "import pandas as pd # conda install pandas\n",
    "import warnings\n",
    "import seaborn as sns # conda install seaborn - Python data visualization library based on matplotlib\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f4b6d4a-7b56-42d3-9570-fbfa823895db",
   "metadata": {},
   "source": [
    "## Load data and take a look at it - all data MUST be numeric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cf6c57f-8ca6-41d2-9d2f-d069dfb9553c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('airbnb new york.csv').sample(frac=1) # returns a random sample of the whole dataframe (frac=1)\n",
    "print('\\n*** Data head\\n')\n",
    "data.head()\n",
    "#print('\\n*** Data describe\\n')\n",
    "#data.describe()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "404d5825-9977-4e36-b672-b27142b40c35",
   "metadata": {},
   "source": [
    "## Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9d1cd6d-6d53-4588-8b85-5f2b597470de",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = data[['neighbourhood_group', 'room_type', 'minimum_nights', 'number_of_reviews', 'reviews_per_month', 'calculated_host_listings_count', 'availability_365']]\n",
    "print('*** Columns\\n', features.head())\n",
    "# the above print display that neighbourhood and room_type have categorical values\n",
    "\n",
    "print('\\n*** Missing values\\n', features.isna().sum()) \n",
    "# isna() return a boolean same-sized object indicating if the values are NA - None or numpy.NaN - and they get mapped to True values.\n",
    "# the above features.isna().sum() shows that 'reviews_per_month' has several thousands od NA (missing) values.\n",
    "\n",
    "features['reviews_per_month'] = features['reviews_per_month'].fillna(0) # fill NA missing values with 0s\n",
    "print('\\n*** Cleaned data\\n', features.isna().sum()) # it shows data is cleaned\n",
    "\n",
    "# get onehot encoding with pd.get_dummies()\n",
    "onehot_neighborhood_group = pd.get_dummies(features['neighbourhood_group'])\n",
    "onehot_room_type = pd.get_dummies(features['room_type'])\n",
    "print('\\n*** onehot encoded data\\n', onehot_neighborhood_group)\n",
    "print('\\n', onehot_room_type)\n",
    "\n",
    "features = features.drop(columns=['neighbourhood_group', 'room_type']) # drop columns with categorical data\n",
    "features = pd.concat([features, onehot_neighborhood_group, onehot_room_type], axis=1) # concatenate dataframe with onehot encoded columns\n",
    "print('\\n*** Processed data\\n', features.head()) # observe updated features \n",
    "\n",
    "targets = data['price'] # get the targets\n",
    "train_size = int(0.7 * len(data)) # 70% od data will be used for training purposes\n",
    "\n",
    "# gets 70% of rows with all columns for X_test, and the remaining 30% of rows with all columns \n",
    "X_train, X_test = features.values[:train_size, :], features.values[train_size:, :]\n",
    "y_train, y_test = targets.values[:train_size], targets.values[train_size:]\n",
    "print('\\nTotal number of columns\\n', len(X_test[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a3d25fa-da34-43da-9427-ba3f303a0731",
   "metadata": {},
   "source": [
    "## Data visualization and analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f2b3012-7477-42c8-9a46-26000766c53b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pairplot() -  this function will create a grid of Axes such that each numeric variable in data will by shared\n",
    "# across the y-axes across a single row and the x-axes across a single column. The diagonal plots are treated\n",
    "# differently: a univariate distribution plot is drawn to show the marginal distribution of the data in each column\n",
    "sns.pairplot(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2a455a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# corr = data.corr() # original data has categorical variables that produce errors\n",
    "corr = features.corr() # corr() - Compute pairwise correlation of columns, excluding NA/null values.\n",
    "\n",
    "cmap = sns.diverging_palette(250, 10, as_cmap=True) # Make a diverging palette between two HUSL colors.\n",
    "\n",
    "# Create a new figure, or activate an existing figure.\n",
    "# in this case the figure call with the figsize argument is used to define the size of the chart area.\n",
    "plt.figure(figsize=(8, 8)) \n",
    "\n",
    "# heatmap() - Plot rectangular data as a color-encoded matrix.\n",
    "sns.heatmap(corr, square=True, cmap=cmap, annot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d572c425-8b93-49d3-a107-91e66450c052",
   "metadata": {},
   "source": [
    "## The Tensorflow 2 Machine Learning Approaches\n",
    "### Linear Regression\n",
    "#### Model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcb6f437",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reused the LinearModel function from the Linear regression notebook\n",
    "class LinearModel:\n",
    "    def __init__(self):\n",
    "        # y_pred = W*X + b\n",
    "        \n",
    "        # self.W = tf.Variable(13.0) - removed for being just one variable\n",
    "        # self.b = tf.Variable(4.0) - removed for being just one variable\n",
    "        \n",
    "        # initializers define the way to set the initial random weights (note plural here) of Keras layers\n",
    "        # we are moving from one variable to a multiple variable (13) linear regression model\n",
    "        self.initializer = tf.keras.initializers.GlorotUniform() \n",
    "\n",
    "    # loss function\n",
    "    def loss(self, y, y_pred):\n",
    "        # calculates the mean absolute error instead of mean squared error\n",
    "        # MAE treats all errors equally, minimizing the impact of outliers on the loss function.\n",
    "        # MSE provides faster convergence BUT has susceptibility to outliers influence and  \n",
    "        #     makes it less suitable for datasets containing anomalies\n",
    "        # https://medium.com/@nirajan.acharya666/choosing-between-mean-squared-error-mse-and-mean-absolute-error-mae-in-regression-a-deep-dive-c16b4eeee603\n",
    "        # return tf.reduce_mean(tf.square(y - y_pred))\n",
    "        return tf.reduce_mean(tf.abs(y - y_pred))\n",
    "        \n",
    "    # train function\n",
    "    def train(self, X, y, lr=0.00001, epochs=20, verbose=True):\n",
    "        \n",
    "        # asarray - converts the input to an array and ensures we are using numpy float32 arrays\n",
    "        print('\\n*** X before array\\n', X)\n",
    "        X = np.asarray(X, dtype=np.float32)\n",
    "        print('\\n*** X after array\\n', X)\n",
    "        \n",
    "        # reshape() - Gives a new shape to an array without changing its data.\n",
    "        print('\\n*** y before array and reshape()\\n', X)\n",
    "        y = np.asarray(y, dtype=np.float32).reshape((-1, 1)) # [1,2,3,4] -> [[1],[2],[3],[4]]      \n",
    "        print('\\n*** y after array and reshape()\\n', X)\n",
    "        \n",
    "        # use the initializer from the constructor above to initialize the multiple features' weights and biases\n",
    "        # LEN(x[0]) - NUMBER OF FEATURES\n",
    "        self.W = tf.Variable(initial_value=self.initializer(shape=(len(X[0]), 1), dtype='float32'))\n",
    "        self.b = tf.Variable(initial_value=self.initializer(shape=(1,), dtype='float32'))\n",
    "        \n",
    "        def train_step():\n",
    "            # GradientTape is a mathematical tool for automatic differentiation (autodiff), which is the core functionality \n",
    "            # of TensorFlow. It does not \"track\" the autodiff, it is a key part of performing the autodiff. It is used to\n",
    "            # record (\"tape\") a sequence of operations performed upon some input and producing some output, so that the output\n",
    "            # can be differentiated with respect to the input (via backpropagation / reverse-mode autodiff) (in order to \n",
    "            # then perform gradient descent optimization). In other words, TensorFlow \"records\" relevant operations executed \n",
    "            # inside the context of a tf.GradientTape onto a \"tape\". TensorFlow then uses that tape to compute the gradients \n",
    "            # of a \"recorded\" computation using reverse mode differentiation.\n",
    "            with tf.GradientTape() as t:\n",
    "                current_loss = self.loss(y, self.predict(X))\n",
    "\n",
    "            # Once you've recorded some operations, use GradientTape.gradient(target, sources) to calculate the gradient of \n",
    "            # some target (often a loss) relative to some source (often the model's variables).\n",
    "            # To get the gradient of loss with respect to both variables, you can pass both as sources to the gradient \n",
    "            # method. The tape is flexible about how sources are passed and will accept any nested combination of lists or\n",
    "            # dictionaries and return the gradient structured the same way (see tf.nest).\n",
    "            # two sources: self.W and self.b return the gradient structured in the same way: dW and db\n",
    "            dW, db = t.gradient(current_loss, [self.W, self.b])\n",
    "            # adjust gradient with the learning rate but because we are using tensors we use assign_sub instead of '-='\n",
    "            self.W.assign_sub(lr * dW) # W -= lr * dW\n",
    "            self.b.assign_sub(lr * db) # b -= lr * db\n",
    "            \n",
    "            return current_loss\n",
    "\n",
    "        # calls train_step() as many times as the number of epochs\n",
    "        for epoch in range(epochs):\n",
    "            current_loss = train_step()\n",
    "            if verbose:\n",
    "                print(f'Epoch {epoch}: loss: {current_loss.numpy()}') # calls numpy() to enable eager execution\n",
    "                # https://analyticsindiamag.com/beginners-guide-to-tensorflow-eager-execution-machine-learning-developers/\n",
    "\n",
    "    def predict(self, X):\n",
    "        # return self.W * X + self.b  - remove due to being just one variable\n",
    "        # [a, b] x [b, c]\n",
    "        # X -> [n_instances, n_features] x [n_features, 1]\n",
    "        return tf.matmul(X, self.W) + self.b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70f7cb0c-3e82-49c4-ab12-f404fb175bad",
   "metadata": {},
   "source": [
    "#### Model instantiation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbcbb5d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearModel()   # creates a model\n",
    "model.train(X_test, y_test, lr=0.00001, epochs=100)   # trains the model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec5aa66e",
   "metadata": {},
   "source": [
    "#### Multi Layer Perceptron model (ANN - Artificial Neural Network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a77135ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# keras is part of tensorflow 2.x\n",
    "# models.Sequential() indicates that every layer we add goes in sequential order, one after another\n",
    "# A model grouping layers into an object with training/inference features.\n",
    "# Sequential groups a linear stack of layers into a tf.keras.Model\n",
    "\n",
    "# Dense(units) implements the operation: output = activation(dot(input, kernel) + bias) where \n",
    "# activation is the element-wise activation function passed as the activation argument, kernel is a weights\n",
    "# matrix created by the layer, and bias is a bias vector created by the layer (only applicable if \n",
    "# use_bias is True). These are all attributes of Dense. units - Positive integer, dimensionality of the output space.\n",
    "\n",
    "model = tf.keras.models.Sequential([\n",
    "    # 1st layer - Dense() Just your regular densely-connected NN layer - 128 dimensions of output space\n",
    "    # Activation function to use. If you don't specify anything, no activation is applied (ie. \"linear\" activation: a(x) = x).\n",
    "    # relu - rectified linear unit activation function. With default values, this returns the standard ReLU \n",
    "    # activation: max(x, 0), the element-wise maximum of 0 and the input tensor.\n",
    "    tf.keras.layers.Dense(128, activation='relu'), \n",
    "    # The Dropout layer randomly sets input units to 0 with a frequency of rate at each step during training time, \n",
    "    # which helps prevent overfitting. Inputs not set to 0 are scaled up by 1/(1 - rate) such that the sum over all inputs is unchanged.\n",
    "    tf.keras.layers.Dropout(0.2), # Dropout 20% of the input\n",
    "    tf.keras.layers.Dense(1)    # 2nd layer - Dense() Just your regular densely-connected NN layer - 1 dimension of output space\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2412c752",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the keras.optimizer is the equivalent of the train_step() function of the linear model\n",
    "# Adam optimization is a stochastic gradient descent method that is based on adaptive estimation of first-order and second-order moments.\n",
    "# USE THIS OPTION BY DEFAULT AND NOT CHANGE IT UNLESS YOU KNOW WHAT YOU ARE DOING\n",
    "adam_optimizer = tf.keras.optimizers.Adam()\n",
    "# \n",
    "loss_fn = tf.keras.losses.MAE   # uses the Mean Absolute Error \n",
    "\n",
    "# The compile() method: specifying a loss, metrics, and an optimizer\n",
    "# To train a model with fit(), you need to specify a loss function, an optimizer, and optionally, some metrics to monitor.\n",
    "model.compile(\n",
    "    optimizer=adam_optimizer, \n",
    "    loss=loss_fn,\n",
    "    metrics=[tf.keras.metrics.MAE]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ee475ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\n*** X_train before array\\n', X_train)\n",
    "X_train = np.asarray(X_train, dtype=np.float32)\n",
    "y_train = np.asarray(y_train, dtype=np.float32)\n",
    "print('\\n*** X_train after array\\n', X_train)\n",
    "model.fit(X_train, y_train, epochs=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e6d291b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\n*** X_test before array\\n', X_test)\n",
    "X_test = np.asarray(X_test, dtype=np.float32)\n",
    "y_test = np.asarray(y_test, dtype=np.float32)\n",
    "print('\\n*** X_test after array\\n', X_test)\n",
    "model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4cdeed5",
   "metadata": {},
   "source": [
    "## Conclusions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf647207-3215-47c1-8363-6dc7a5ca1d11",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
