{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2e9b3e6d-d507-46e3-806a-1e5a9f7f3eaf",
   "metadata": {},
   "source": [
    "# 1.4. AirBnB Project Template\n",
    "\n",
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28ed7c80-1bbd-4db5-8113-67f5ba78551b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "# MULTI LAYER PERCEPTRON - ARTIFICIAL NEURAL NETWORK - OPTIMIZED NETWORK\n",
    "# Added custom metrics\n",
    "# Putting all together for Production\n",
    "#\n",
    "\n",
    "import numpy as np # conda install numpy\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt # conda install matplotlib\n",
    "import pandas as pd # conda install pandas\n",
    "import warnings\n",
    "import seaborn as sns # conda install seaborn - Python data visualization library based on matplotlib\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f4b6d4a-7b56-42d3-9570-fbfa823895db",
   "metadata": {},
   "source": [
    "## Load data and take a look at it - all data MUST be numeric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cf6c57f-8ca6-41d2-9d2f-d069dfb9553c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('airbnb new york.csv').sample(frac=1) # returns a random sample of the whole dataframe (frac=1)\n",
    "data.head()\n",
    "\n",
    "#data.describe()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "404d5825-9977-4e36-b672-b27142b40c35",
   "metadata": {},
   "source": [
    "## Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9d1cd6d-6d53-4588-8b85-5f2b597470de",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = data[['neighbourhood_group', 'room_type', 'minimum_nights', 'number_of_reviews', 'reviews_per_month', 'calculated_host_listings_count', 'availability_365']]\n",
    "# display to lookout for categorical values\n",
    "print('*** Columns\\n', features.head())\n",
    "\n",
    "# isna() return a boolean same-sized object indicating if the values are NA - None or numpy.NaN - and they get mapped to True values.\n",
    "# the above features.isna().sum() shows that 'reviews_per_month' has several thousands od NA (missing) values.\n",
    "print('\\n*** Missing values\\n', features.isna().sum()) \n",
    "\n",
    "features['reviews_per_month'] = features['reviews_per_month'].fillna(0) # fill NA missing values with 0s\n",
    "print('\\n*** Cleaned data\\n', features.isna().sum()) # it shows data is cleaned\n",
    "\n",
    "# get onehot encoding with pd.get_dummies()\n",
    "onehot_neighborhood_group = pd.get_dummies(features['neighbourhood_group'])\n",
    "onehot_room_type = pd.get_dummies(features['room_type'])\n",
    "\n",
    "print('\\n*** onehot encoded data\\n')\n",
    "print('\\n', onehot_neighborhood_group)\n",
    "print('\\n', onehot_room_type)\n",
    "\n",
    "features = features.drop(columns=['neighbourhood_group', 'room_type']) # drop columns with categorical data\n",
    "features = pd.concat([features, onehot_neighborhood_group, onehot_room_type], axis=1) # concatenate dataframe with onehot encoded columns\n",
    "print('\\n*** Processed data\\n', features.head()) # observe updated features \n",
    "\n",
    "targets = data['price'] # get the targets\n",
    "train_size = int(0.7 * len(data)) # 70% od data will be used for training purposes\n",
    "\n",
    "# gets 70% of rows with all columns for X_test, and the remaining 30% of rows with all columns \n",
    "X_train, X_test = features.values[:train_size, :], features.values[train_size:, :]\n",
    "y_train, y_test = targets.values[:train_size], targets.values[train_size:]\n",
    "print('\\nTotal number of columns\\n', len(X_test[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a3d25fa-da34-43da-9427-ba3f303a0731",
   "metadata": {},
   "source": [
    "## Data visualization and analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f2b3012-7477-42c8-9a46-26000766c53b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pairplot() -  this function will create a grid of Axes such that each numeric variable in data will by shared\n",
    "# across the y-axes across a single row and the x-axes across a single column. The diagonal plots are treated\n",
    "# differently: a univariate distribution plot is drawn to show the marginal distribution of the data in each column\n",
    "sns.pairplot(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2a455a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# corr = data.corr() # original data has categorical variables that produce errors\n",
    "corr = features.corr() # corr() - Compute pairwise correlation of columns, excluding NA/null values.\n",
    "\n",
    "cmap = sns.diverging_palette(250, 10, as_cmap=True) # Make a diverging palette between two HUSL colors.\n",
    "\n",
    "# Create a new figure, or activate an existing figure.\n",
    "# in this case the figure call with the figsize argument is used to define the size of the chart area.\n",
    "plt.figure(figsize=(8, 8)) \n",
    "\n",
    "# heatmap() - Plot rectangular data as a color-encoded matrix.\n",
    "sns.heatmap(corr, square=True, cmap=cmap, annot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec5aa66e",
   "metadata": {},
   "source": [
    "## Multi Layer Perceptron model (ANN - Artificial Neural Network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a77135ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# keras is part of tensorflow 2.x\n",
    "# models.Sequential() indicates that every layer we add goes in sequential order, one after another\n",
    "# A model grouping layers into an object with training/inference features.\n",
    "# Sequential groups a linear stack of layers into a tf.keras.Model\n",
    "\n",
    "# Dense(units) implements the operation: output = activation(dot(input, kernel) + bias) where \n",
    "# activation is the element-wise activation function passed as the activation argument, kernel is a weights\n",
    "# matrix created by the layer, and bias is a bias vector created by the layer (only applicable if \n",
    "# use_bias is True). These are all attributes of Dense. units - Positive integer, dimensionality of the output space.\n",
    "model = tf.keras.models.Sequential([\n",
    "    \n",
    "    # 1st layer - Dense() Just your regular densely-connected NN layer - 128 dimensions of output space\n",
    "    # Activation function to use. If you don't specify anything, no activation is applied (ie. \"linear\" activation: a(x) = x).\n",
    "    # relu - rectified linear unit activation function. With default values, this returns the standard ReLU \n",
    "    # activation: max(x, 0), the element-wise maximum of 0 and the input tensor.\n",
    "    tf.keras.layers.Dense(128, activation='relu'), \n",
    "    \n",
    "    # The Dropout layer randomly sets input units to ZERO with a frequency of rate at each step during training time, \n",
    "    # WHICH HELPS TO PREVENT OVERFITTING. Inputs not set to 0 are scaled up by 1/(1 - rate) such that the sum over all inputs is unchanged.\n",
    "    tf.keras.layers.Dropout(0.2), # Dropout 20% of the input\n",
    "    \n",
    "    tf.keras.layers.Dense(1)    # 2nd layer - Dense() Just your regular densely-connected NN layer - 1 dimension of output space\n",
    "])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2412c752",
   "metadata": {},
   "outputs": [],
   "source": [
    "# more metrics for better understanding\n",
    "def R_squared(y_true, y_pred):\n",
    "    # note use of .reduce_sum and .square tendorflow functions\n",
    "    residual = tf.reduce_sum(tf.square(y_true - y_pred))\n",
    "    # note use of .reduce_mean\n",
    "    total = tf.reduce_sum(tf.square(y_true - tf.reduce_mean(y_true)))\n",
    "    r2 = 1.0 - residual / total\n",
    "    return r2\n",
    "\n",
    "# the keras.optimizer is the equivalent of the train_step() function of the linear model\n",
    "# Adam optimization is a stochastic gradient descent method that is based on adaptive estimation of first-order and second-order moments.\n",
    "# USE THIS OPTION BY DEFAULT AND NOT CHANGE IT UNLESS YOU KNOW WHAT YOU ARE DOING\n",
    "adam_optimizer = tf.keras.optimizers.Adam()\n",
    "# \n",
    "loss_fn = tf.keras.losses.MAE   # uses the Mean Absolute Error \n",
    "\n",
    "# The compile() method: specifying a loss, metrics, and an optimizer\n",
    "# To train a model with fit(), you need to specify a loss function, an optimizer, and optionally, some metrics to monitor.\n",
    "model.compile(\n",
    "    optimizer=adam_optimizer, \n",
    "    loss=loss_fn,\n",
    "    #ADDED METRICS HERE\n",
    "    metrics=[\n",
    "        tf.keras.metrics.MAE,\n",
    "        tf.keras.metrics.MSE,   # mean squared error\n",
    "        R_squared, # result between -1 and +1, < 0 => useless, 0 and 1 => better close to 1\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ee475ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reformats boolean data to float32\n",
    "X_train = np.asarray(X_train, dtype=np.float32)\n",
    "y_train = np.asarray(y_train, dtype=np.float32)\n",
    "\n",
    "# fit() is for TRAINING THE MODEL with the given inputs (and corresponding training labels).\n",
    "model.fit(X_train, y_train, epochs=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e6d291b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the model's architecture\n",
    "model.summary()\n",
    "\n",
    "# evaluate() is for evaluating the already trained model using the validation (or test) data and the corresponding labels. \n",
    "# Returns the loss value and metrics values for the model.\n",
    "X_test = np.asarray(X_test, dtype=np.float32)\n",
    "y_test = np.asarray(y_test, dtype=np.float32)\n",
    "\n",
    "# We EVALUATE THE MODEL on the test data via evaluate():\n",
    "print('\\nEvaluate the model on the test data')\n",
    "model.evaluate(X_test, y_test)\n",
    "\n",
    "\n",
    "# An entire model can be saved in three different file formats (the new .keras format and two legacy formats: SavedModel,\n",
    "# and HDF5). Saving a model as path/to/model.keras automatically saves in the latest format.\n",
    "# Saving a fully-functional model is very usefulâ€”you can load them in TensorFlow.js (Saved Model, HDF5) and then train and\n",
    "# run them in web browsers, or convert them to run on mobile devices using TensorFlow Lite (Saved Model, HDF5)\n",
    "# it saves de model,its architecture, and weights in 'this' notebook's folder\n",
    "model.save('rbnb_model.h5')\n",
    "\n",
    "# SIMULATE THE MODEL BEING USED ON A WEB API - load the saved model\n",
    "loaded_model = tf.keras.models.load_model('rbnb_model.h5', custom_objects={\"R_squared\": R_squared})\n",
    "\n",
    "print('\\nCompare loaded_model vs model from above\\n')\n",
    "print(loaded_model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0be2edd2",
   "metadata": {},
   "source": [
    "#### Making some predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40fa0ef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\nPREDICTION - ORIGINAL MODULE\\n')\n",
    "print(model.predict(X_test[:2]))\n",
    "\n",
    "print('\\nPREDICTION - LOADED MODULE\\n')\n",
    "print(loaded_model(X_test[:2]))\n",
    "\n",
    "print('\\nACTUAL DATA\\n')\n",
    "print(y_test[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4cdeed5",
   "metadata": {},
   "source": [
    "## Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93604baf",
   "metadata": {},
   "source": [
    "- Sometimes the dataset limits us with regards to results\n",
    "    * Features not well correlated to the predicted values\n",
    "- Data preprocessing and analysis is important - TensorFlow does not live in a bubble, it's a tool\n",
    "    * It must work in conjunction with other tools\n",
    "    * Seaborn, pandas, etc.\n",
    "- TF 2 simplifies many things: no more placeholders, we do have eager execution, we do have .numpy(), no more sessions, Keras has a bigger role\n",
    "- ANNs are very sensitive to hyperparameter choice\n",
    "- Running on GPU can help, but it's not a must\n",
    "- I CAN DO IT!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
