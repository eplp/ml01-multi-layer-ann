{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2e9b3e6d-d507-46e3-806a-1e5a9f7f3eaf",
   "metadata": {},
   "source": [
    "# 1.4. AirBnB Project Template\n",
    "\n",
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28ed7c80-1bbd-4db5-8113-67f5ba78551b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # conda install numpy\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt # conda install matplotlib\n",
    "import pandas as pd # conda install pandas\n",
    "import warnings\n",
    "import seaborn as sns # conda install seaborn - Python data visualization library based on matplotlib\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f4b6d4a-7b56-42d3-9570-fbfa823895db",
   "metadata": {},
   "source": [
    "## Load data and take a look at it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cf6c57f-8ca6-41d2-9d2f-d069dfb9553c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('airbnb new york.csv').sample(frac=1) # returns a random sample of the whole dataframe (frac=1)\n",
    "print('\\n*** Data head\\n')\n",
    "data.head()\n",
    "print('\\n*** Data describe\\n')\n",
    "data.describe()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "404d5825-9977-4e36-b672-b27142b40c35",
   "metadata": {},
   "source": [
    "## Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9d1cd6d-6d53-4588-8b85-5f2b597470de",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = data[['neighbourhood_group', 'room_type', 'minimum_nights', 'number_of_reviews', 'reviews_per_month', 'calculated_host_listings_count', 'availability_365']]\n",
    "print('*** Columns\\n', features.head())\n",
    "# the above print display that neighbourhood and room_type have categorical values\n",
    "\n",
    "print('\\n*** Missing values\\n', features.isna().sum()) \n",
    "# isna() return a boolean same-sized object indicating if the values are NA - None or numpy.NaN - and they get mapped to True values.\n",
    "# the above features.isna() shows that 'reviews_per_month' has several thousands od NA (missing) values.\n",
    "\n",
    "features['reviews_per_month'] = features['reviews_per_month'].fillna(0) # fill NA missing values with 0s\n",
    "print('\\n*** Cleaned data\\n', features.isna().sum()) # it shows data is cleaned\n",
    "\n",
    "# get onehot encoding with pd.get_dummies()\n",
    "onehot_neighborhood_group = pd.get_dummies(features['neighbourhood_group'])\n",
    "onehot_room_type = pd.get_dummies(features['room_type'])\n",
    "print('\\n*** onehot encoded data\\n', onehot_neighborhood_group)\n",
    "print(onehot_room_type)\n",
    "\n",
    "features = features.drop(columns=['neighbourhood_group', 'room_type']) # drop columns with categorical data\n",
    "features = pd.concat([features, onehot_neighborhood_group, onehot_room_type], axis=1) # concatenate dataframe with onehot encoded columns\n",
    "print('\\n*** Processed data\\n', features.head()) # observe updated features \n",
    "\n",
    "targets = data['price'] # get the targets\n",
    "\n",
    "train_size = int(0.7 * len(data)) # 70% od data will be used for training purposes\n",
    "\n",
    "# gets 70% of rows with all columns for X_train, and the remaining 30% of rows with all columns \n",
    "X_train, X_test = features.values[:train_size, :], features.values[train_size:, :]\n",
    "y_train, y_test = targets.values[:train_size], targets.values[train_size:]\n",
    "print('\\nTotal number of columns\\n', len(X_train[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a3d25fa-da34-43da-9427-ba3f303a0731",
   "metadata": {},
   "source": [
    "## Data visualization and analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f2b3012-7477-42c8-9a46-26000766c53b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2a455a2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d572c425-8b93-49d3-a107-91e66450c052",
   "metadata": {},
   "source": [
    "## The Tensorflow 2 Machine Learning Approaches\n",
    "### Linear Regression\n",
    "#### Model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcb6f437",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reused the LinearModel function from the Linear regression notebook\n",
    "class LinearModel:\n",
    "    def __init__(self):\n",
    "        # y_pred = W*X + b\n",
    "        \n",
    "        # self.W = tf.Variable(13.0)\n",
    "        # self.b = tf.Variable(4.0)\n",
    "        \n",
    "        # initializers define the way to set the initial random weights (note plural here) of Keras layers\n",
    "        # we are moving from one variable to a multiple variable (13) linear regression model\n",
    "        self.initializer = tf.keras.initializers.GlorotUniform() \n",
    "\n",
    "    # loss function\n",
    "    def loss(self, y, y_pred):\n",
    "        # calculates the mean absolute error instead of mean squared error\n",
    "        # MAE treats all errors equally, minimizing the impact of outliers on the loss function.\n",
    "        # MSE provides faster convergence BUT has susceptibility to outliers influence and  \n",
    "        #     makes it less suitable for datasets containing anomalies\n",
    "        # https://medium.com/@nirajan.acharya666/choosing-between-mean-squared-error-mse-and-mean-absolute-error-mae-in-regression-a-deep-dive-c16b4eeee603\n",
    "        # return tf.reduce_mean(tf.square(y - y_pred))\n",
    "        return tf.reduce_mean(tf.abs(y - y_pred))\n",
    "        \n",
    "    # train function\n",
    "    def train(self, X, y, lr=0.00001, epochs=20, verbose=True):\n",
    "        \n",
    "        # asarray - converts the input to an array and ensures we are using numpy float32 arrays\n",
    "        print('\\n*** X before array\\n', X)\n",
    "        X = np.asarray(X, dtype=np.float32)\n",
    "        print('\\n*** X after array\\n', X)\n",
    "        \n",
    "        # reshape() - Gives a new shape to an array without changing its data.\n",
    "        print('\\n*** y before array and reshape()\\n', X)\n",
    "        y = np.asarray(y, dtype=np.float32).reshape((-1, 1)) # [1,2,3,4] -> [[1],[2],[3],[4]]      \n",
    "        print('\\n*** y after array and reshape()\\n', X)\n",
    "        \n",
    "        # use the initializer from the constructor above to initialize the multiple features' weights and biases\n",
    "        # LEN(x[0]) - NUMBER OF FEATURES\n",
    "        self.W = tf.Variable(initial_value=self.initializer(shape=(len(X[0]), 1), dtype='float32'))\n",
    "        self.b = tf.Variable(initial_value=self.initializer(shape=(1,), dtype='float32'))\n",
    "        \n",
    "        def train_step():\n",
    "            with tf.GradientTape() as t:\n",
    "                current_loss = self.loss(y, self.predict(X))\n",
    "\n",
    "            dW, db = t.gradient(current_loss, [self.W, self.b])\n",
    "            self.W.assign_sub(lr * dW) # W -= lr * dW\n",
    "            self.b.assign_sub(lr * db) # b -= lr * db\n",
    "            \n",
    "            return current_loss\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            current_loss = train_step()\n",
    "            if verbose:\n",
    "                print(f'Epoch {epoch}: loss: {current_loss.numpy()}') # <3 eager execution\n",
    "\n",
    "    def predict(self, X):\n",
    "        # return self.W * X + self.b  - remove due to being just one variable\n",
    "        # [a, b] x [b, c]\n",
    "        # X -> [n_instances, n_features] x [n_features, 1]\n",
    "        return tf.matmul(X, self.W) + self.b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70f7cb0c-3e82-49c4-ab12-f404fb175bad",
   "metadata": {},
   "source": [
    "#### Model instantiation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbcbb5d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearModel()\n",
    "model.train(X_train, y_train, lr=0.00001, epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec5aa66e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f4cdeed5",
   "metadata": {},
   "source": [
    "## Conclusions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf647207-3215-47c1-8363-6dc7a5ca1d11",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
